#!/bin/bash

#SBATCH --job-name={{ job_name }}
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gres=gpu:{{ total_gpus }}
{% if total_gpus == 8 %}
#SBATCH --exclusive
{% endif %}
#SBATCH --output="{{ output_dir }}/slurm/job_%j.log"
#SBATCH --error="{{ output_dir }}/slurm/job_%j.log"

set -euo pipefail

export PROJECT_DIR="{{ project_dir }}"
export CONFIG_DIR="{{ config_dir }}"
export OUTPUT_DIR="{{ output_dir }}"
export HF_CACHE_DIR="{{ hf_cache_dir }}"
export HF_HUB_OFFLINE={{ 1 if hf_hub_offline else 0 }}
export HF_HOME="$HF_CACHE_DIR"

export TOTAL_GPUS={{ total_gpus }}
export NUM_TRAIN_GPUS={{ train_gpus }}
export NUM_INFER_GPUS={{ infer_gpus }}

mkdir -p "$OUTPUT_DIR/slurm" "$OUTPUT_DIR/torchrun"

if [ -f "$PROJECT_DIR/.env" ]; then
  # shellcheck disable=SC1091
  source "$PROJECT_DIR/.env"
fi
# shellcheck disable=SC1091
source "$PROJECT_DIR/.venv/bin/activate"

export CUDA_DEVICE_ORDER=PCI_BUS_ID
export PYTHONUNBUFFERED=1
export OMP_NUM_THREADS=1
export MASTER_ADDR=127.0.0.1

export JOB_TMP="${SLURM_TMPDIR:-/tmp/${USER:-user}/${SLURM_JOB_ID:-nojob}}"
mkdir -p "$JOB_TMP"

pick_free_ports() {
  local py_bin=""
  if command -v python3 >/dev/null 2>&1; then
    py_bin="python3"
  elif command -v python >/dev/null 2>&1; then
    py_bin="python"
  else
    echo "python3 (or python) is required for picking free ports." >&2
    exit 1
  fi
  "$py_bin" - "$1" <<'PY'
import socket
import sys

n = int(sys.argv[1])
sockets = []
ports = []
for _ in range(n):
    s = socket.socket()
    s.bind(("127.0.0.1", 0))
    sockets.append(s)
    ports.append(str(s.getsockname()[1]))
print(" ".join(ports))
for s in sockets:
    s.close()
PY
}

missing_port_count=0
[ -z "${INFER_PORT:-}" ] && missing_port_count=$((missing_port_count + 1))
[ -z "${RDZV_PORT:-}" ] && missing_port_count=$((missing_port_count + 1))
{% if nccl_enabled %}
[ -z "${WEIGHT_BROADCAST_PORT:-}" ] && missing_port_count=$((missing_port_count + 1))
{% endif %}

if [ "$missing_port_count" -gt 0 ]; then
  read -r -a _PICKED_PORTS < <(pick_free_ports "$missing_port_count")
  _picked_idx=0
  if [ -z "${INFER_PORT:-}" ]; then
    INFER_PORT="${_PICKED_PORTS[$_picked_idx]}"
    _picked_idx=$((_picked_idx + 1))
  fi
  if [ -z "${RDZV_PORT:-}" ]; then
    RDZV_PORT="${_PICKED_PORTS[$_picked_idx]}"
    _picked_idx=$((_picked_idx + 1))
  fi
{% if nccl_enabled %}
  if [ -z "${WEIGHT_BROADCAST_PORT:-}" ]; then
    WEIGHT_BROADCAST_PORT="${_PICKED_PORTS[$_picked_idx]}"
    _picked_idx=$((_picked_idx + 1))
  fi
{% endif %}
fi

export INFER_PORT RDZV_PORT
{% if nccl_enabled %}
export WEIGHT_BROADCAST_PORT
{% endif %}

INFER_LATEST_LOG="$OUTPUT_DIR/slurm/latest_infer_node_rank_0.log"
INFER_JOB_LOG="$OUTPUT_DIR/slurm/job_${SLURM_JOB_ID}_infer_node_rank_0.log"
ORCH_LATEST_LOG="$OUTPUT_DIR/slurm/latest_orchestrator.log"
ORCH_JOB_LOG="$OUTPUT_DIR/slurm/job_${SLURM_JOB_ID}_orchestrator.log"
TRAIN_LATEST_LOG="$OUTPUT_DIR/slurm/latest_train_node_rank_0.log"
TRAIN_JOB_LOG="$OUTPUT_DIR/slurm/job_${SLURM_JOB_ID}_train_node_rank_0.log"
export INFER_LATEST_LOG INFER_JOB_LOG ORCH_LATEST_LOG ORCH_JOB_LOG TRAIN_LATEST_LOG TRAIN_JOB_LOG

INFER_XDG_CACHE_HOME="$JOB_TMP/xdg_infer"
INFER_TRITON_CACHE_DIR="$JOB_TMP/triton_infer"
INFER_TORCHINDUCTOR_CACHE_DIR="$JOB_TMP/torchinductor_infer"
ORCH_XDG_CACHE_HOME="$JOB_TMP/xdg_orch"
ORCH_TRITON_CACHE_DIR="$JOB_TMP/triton_orch"
ORCH_TORCHINDUCTOR_CACHE_DIR="$JOB_TMP/torchinductor_orch"
TRAIN_XDG_CACHE_HOME="$JOB_TMP/xdg_train"
TRAIN_TRITON_CACHE_DIR="$JOB_TMP/triton_train"
TRAIN_TORCHINDUCTOR_CACHE_DIR="$JOB_TMP/torchinductor_train"
export \
  INFER_XDG_CACHE_HOME INFER_TRITON_CACHE_DIR INFER_TORCHINDUCTOR_CACHE_DIR \
  ORCH_XDG_CACHE_HOME ORCH_TRITON_CACHE_DIR ORCH_TORCHINDUCTOR_CACHE_DIR \
  TRAIN_XDG_CACHE_HOME TRAIN_TRITON_CACHE_DIR TRAIN_TORCHINDUCTOR_CACHE_DIR

cleanup() {
  if [ "${CLEANUP_DONE:-0}" -eq 1 ]; then
    return
  fi
  CLEANUP_DONE=1
  [ -n "${ORCH_SRUN_PID:-}" ] && kill "${ORCH_SRUN_PID}" 2>/dev/null || true
  [ -n "${INFER_SRUN_PID:-}" ] && kill "${INFER_SRUN_PID}" 2>/dev/null || true

  rm -rf -- \
    "$INFER_XDG_CACHE_HOME" "$INFER_TRITON_CACHE_DIR" "$INFER_TORCHINDUCTOR_CACHE_DIR" \
    "$ORCH_XDG_CACHE_HOME" "$ORCH_TRITON_CACHE_DIR" "$ORCH_TORCHINDUCTOR_CACHE_DIR" \
    "$TRAIN_XDG_CACHE_HOME" "$TRAIN_TRITON_CACHE_DIR" "$TRAIN_TORCHINDUCTOR_CACHE_DIR" \
    2>/dev/null || true
}
trap cleanup EXIT INT TERM

echo "Starting inference (vLLM) on 127.0.0.1:${INFER_PORT} using ${NUM_INFER_GPUS} GPU(s)"
srun --overlap --ntasks=1 --gres=gpu:"$NUM_INFER_GPUS" bash -lc '
  set -euo pipefail
  [ -f "$PROJECT_DIR/.env" ] && source "$PROJECT_DIR/.env"
  source "$PROJECT_DIR/.venv/bin/activate"

  export XDG_CACHE_HOME="$INFER_XDG_CACHE_HOME"
  export TRITON_CACHE_DIR="$INFER_TRITON_CACHE_DIR"
  export TORCHINDUCTOR_CACHE_DIR="$INFER_TORCHINDUCTOR_CACHE_DIR"
  mkdir -p "$XDG_CACHE_HOME" "$TRITON_CACHE_DIR" "$TORCHINDUCTOR_CACHE_DIR"

  export VLLM_WORKER_MULTIPROC_METHOD=spawn
  export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:False"

  uv run inference \
    @ "$CONFIG_DIR/inference.toml" \
    --server.host 127.0.0.1 \
    --server.port "$INFER_PORT" \
    2>&1 | tee "$INFER_LATEST_LOG" "$INFER_JOB_LOG"
' &
INFER_SRUN_PID=$!

echo "Starting orchestrator (expected to wait for vLLM readiness)"
srun --overlap --ntasks=1 bash -lc '
  set -euo pipefail
  [ -f "$PROJECT_DIR/.env" ] && source "$PROJECT_DIR/.env"
  source "$PROJECT_DIR/.venv/bin/activate"

  export XDG_CACHE_HOME="$ORCH_XDG_CACHE_HOME"
  export TRITON_CACHE_DIR="$ORCH_TRITON_CACHE_DIR"
  export TORCHINDUCTOR_CACHE_DIR="$ORCH_TORCHINDUCTOR_CACHE_DIR"
  mkdir -p "$XDG_CACHE_HOME" "$TRITON_CACHE_DIR" "$TORCHINDUCTOR_CACHE_DIR"

  uv run orchestrator \
    @ "$CONFIG_DIR/orchestrator.toml" \
    --client.base-url "http://127.0.0.1:${INFER_PORT}/v1" \
{% if nccl_enabled %}
    --weight_broadcast.host 127.0.0.1 \
    --weight_broadcast.port "$WEIGHT_BROADCAST_PORT" \
{% endif %}
    2>&1 | tee "$ORCH_LATEST_LOG" "$ORCH_JOB_LOG"
' &
ORCH_SRUN_PID=$!

echo "Starting training using ${NUM_TRAIN_GPUS} GPU(s)"
set +e
srun --overlap --ntasks=1 --gres=gpu:"$NUM_TRAIN_GPUS" bash -lc '
  set -euo pipefail
  [ -f "$PROJECT_DIR/.env" ] && source "$PROJECT_DIR/.env"
  source "$PROJECT_DIR/.venv/bin/activate"

  export XDG_CACHE_HOME="$TRAIN_XDG_CACHE_HOME"
  export TRITON_CACHE_DIR="$TRAIN_TRITON_CACHE_DIR"
  export TORCHINDUCTOR_CACHE_DIR="$TRAIN_TORCHINDUCTOR_CACHE_DIR"
  mkdir -p "$XDG_CACHE_HOME" "$TRITON_CACHE_DIR" "$TORCHINDUCTOR_CACHE_DIR"

  export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

  uv run torchrun \
    --nnodes=1 \
    --nproc-per-node "$NUM_TRAIN_GPUS" \
    --node-rank=0 \
    --rdzv-endpoint=127.0.0.1:$RDZV_PORT \
    --rdzv-id=job_$SLURM_JOB_ID \
    --log-dir "$OUTPUT_DIR/torchrun" \
    --tee=3 \
    --redirects=3 \
    --local-ranks-filter "$(seq -s, 0 $((NUM_TRAIN_GPUS - 1)))" \
    -m prime_rl.trainer.rl.train \
    @ "$CONFIG_DIR/trainer.toml" \
{% if nccl_enabled %}
    --weight_broadcast.host 127.0.0.1 \
    --weight_broadcast.port "$WEIGHT_BROADCAST_PORT" \
{% endif %}
    2>&1 | tee "$TRAIN_LATEST_LOG" "$TRAIN_JOB_LOG"
'
TRAIN_EXIT_CODE=$?
set -e

cleanup
wait "${INFER_SRUN_PID:-}" 2>/dev/null || true
wait "${ORCH_SRUN_PID:-}" 2>/dev/null || true

exit "$TRAIN_EXIT_CODE"
