#!/bin/bash

#SBATCH --job-name={{ job_name }}
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --gres=gpu:{{ total_gpus }}
{% if total_gpus == 8 %}
#SBATCH --exclusive
{% else %}
#SBATCH --cpus-per-gpu={{ cpus_per_gpu }}
{% endif %}
#SBATCH --export=ALL
#SBATCH --output="{{ output_dir }}/slurm/job_%j.log"
#SBATCH --error="{{ output_dir }}/slurm/job_%j.log"

set -euo pipefail

export PROJECT_DIR="{{ project_dir }}"
export CONFIG_DIR="{{ config_dir }}"
export OUTPUT_DIR="{{ output_dir }}"
export HF_CACHE_DIR="{{ hf_cache_dir }}"
export HF_HUB_OFFLINE={{ 1 if hf_hub_offline else 0 }}
export HF_HOME="$HF_CACHE_DIR"
export MEDARC_SINGLE_GPU={{ 1 if single_gpu else 0 }}

mkdir -p "$OUTPUT_DIR/slurm" "$OUTPUT_DIR/torchrun"

if [ -f "$PROJECT_DIR/.env" ]; then
  set -a
  # shellcheck disable=SC1091
  source "$PROJECT_DIR/.env"
  set +a
fi
# shellcheck disable=SC1091
source "$PROJECT_DIR/.venv/bin/activate"

export CUDA_DEVICE_ORDER=PCI_BUS_ID
export PYTHONUNBUFFERED=1
export OMP_NUM_THREADS=1

python -m medarc_rl.launchers.rl_local @ "$CONFIG_DIR/rl.toml"
